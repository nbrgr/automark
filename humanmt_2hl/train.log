2019-12-08 17:55:21,782 Hello! This is AutoMark. For all your Automatic Marking needs.
2019-12-08 17:55:21,788 Total params: 87200
2019-12-08 17:55:21,790 Trainable parameters: ['marking_head.fc1.bias', 'marking_head.fc1.weight', 'marking_head.fc2.bias', 'marking_head.fc2.weight', 'marking_head.prediction.weight']
2019-12-08 17:55:25,375 cfg.bert.path                      : bert-base-multilingual-cased
2019-12-08 17:55:25,376 cfg.data.source                    : .en
2019-12-08 17:55:25,376 cfg.data.target                    : .hyp
2019-12-08 17:55:25,376 cfg.data.marking                   : .ann
2019-12-08 17:55:25,376 cfg.data.raw_train                 : data/markings
2019-12-08 17:55:25,376 cfg.data.train                     : data/markings.tok
2019-12-08 17:55:25,376 cfg.data.raw_dev                   : data/user_mark
2019-12-08 17:55:25,376 cfg.data.dev                       : data/user_mark.tok
2019-12-08 17:55:25,376 cfg.model.hidden_dimension         : 100
2019-12-08 17:55:25,376 cfg.model.activation               : relu
2019-12-08 17:55:25,377 cfg.model.freeze_bert              : True
2019-12-08 17:55:25,377 cfg.model.head_bias                : False
2019-12-08 17:55:25,377 cfg.train.bert_lr                  : 0.0003
2019-12-08 17:55:25,377 cfg.train.lr                       : 0.0003
2019-12-08 17:55:25,377 cfg.train.optimizer                : adam
2019-12-08 17:55:25,377 cfg.train.batch_size               : 32
2019-12-08 17:55:25,377 cfg.train.epochs                   : 25
2019-12-08 17:55:25,377 cfg.train.seed                     : 41
2019-12-08 17:55:25,377 cfg.train.model_dir                : humanmt_2hl
2019-12-08 17:55:25,377 cfg.train.shuffle                  : True
2019-12-08 17:55:25,377 cfg.train.cuda                     : True
2019-12-08 17:55:25,377 cfg.train.early_stopping_metric    : eval_metric
2019-12-08 17:55:25,377 cfg.train.overwrite                : True
2019-12-08 17:55:25,378 cfg.train.normalization            : tokens
2019-12-08 17:55:25,378 cfg.train.bad_weight               : 10.0
2019-12-08 17:55:25,378 cfg.train.validation_freq          : 100
2019-12-08 17:55:25,378 cfg.train.logging_freq             : 10
2019-12-08 17:55:25,378 cfg.train.weighting                : constant
2019-12-08 17:55:25,378 cfg.train.eval_batch_size          : 128
2019-12-08 17:55:25,378 cfg.train.eval_metric              : f1_prod
2019-12-08 17:55:25,378 cfg.generate.batch_size            : 2
2019-12-08 17:55:25,380 AutoMark(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (marking_head): MarkingHead(
    (fc1): Linear(in_features=768, out_features=100, bias=True)
    (fc2): Linear(in_features=100, out_features=100, bias=True)
    (prediction): Linear(in_features=100, out_features=2, bias=False)
  )
)
2019-12-08 17:55:25,380 EPOCH 1
2019-12-08 17:55:26,321 Epoch   1 Step:       10 Batch Loss:     1.596360 Ones: 0.00 Accuracy: 0.16 Tokens per Sec:     8748, Lr: 0.000300
2019-12-08 17:55:27,087 Epoch   1 Step:       20 Batch Loss:     1.460565 Ones: 0.00 Accuracy: 0.13 Tokens per Sec:    23467, Lr: 0.000300
2019-12-08 17:55:27,716 Epoch   1 Step:       30 Batch Loss:     1.304218 Ones: 0.00 Accuracy: 0.10 Tokens per Sec:    40330, Lr: 0.000300
2019-12-08 17:55:27,895 Epoch   1: total training loss 51.79
2019-12-08 17:55:27,895 EPOCH 2
2019-12-08 17:55:28,345 Epoch   2 Step:       40 Batch Loss:     1.591873 Ones: 0.00 Accuracy: 0.19 Tokens per Sec:    12537, Lr: 0.000300
2019-12-08 17:55:29,124 Epoch   2 Step:       50 Batch Loss:     1.517353 Ones: 0.02 Accuracy: 0.19 Tokens per Sec:    18863, Lr: 0.000300
2019-12-08 17:55:29,766 Epoch   2 Step:       60 Batch Loss:     1.519369 Ones: 0.24 Accuracy: 0.38 Tokens per Sec:    34998, Lr: 0.000300
2019-12-08 17:55:30,172 Epoch   2: total training loss 48.43
2019-12-08 17:55:30,172 EPOCH 3
2019-12-08 17:55:30,462 Epoch   3 Step:       70 Batch Loss:     1.506284 Ones: 0.14 Accuracy: 0.31 Tokens per Sec:    10870, Lr: 0.000300
2019-12-08 17:55:31,020 Epoch   3 Step:       80 Batch Loss:     1.661877 Ones: 0.34 Accuracy: 0.47 Tokens per Sec:    17470, Lr: 0.000300
2019-12-08 17:55:31,814 Epoch   3 Step:       90 Batch Loss:     1.378662 Ones: 0.35 Accuracy: 0.46 Tokens per Sec:    24811, Lr: 0.000300
2019-12-08 17:55:32,459 Epoch   3: total training loss 46.30
2019-12-08 17:55:32,459 EPOCH 4
2019-12-08 17:55:32,527 Epoch   4 Step:      100 Batch Loss:     1.378283 Ones: 0.27 Accuracy: 0.40 Tokens per Sec:    11958, Lr: 0.000300
2019-12-08 17:55:35,312 Hooray! New best validation result [eval_metric]!
2019-12-08 17:55:35,312 Saving new checkpoint.
2019-12-08 17:55:42,177 Validation result at epoch   4, step      100: f1_prod:   0.11, loss:   0.9209, ones:   0.3647, f1_0:   0.2184, f1_1:   0.5172,f1_prd:   0.1129, duration: 9.6500s
2019-12-08 17:55:42,913 Epoch   4 Step:      110 Batch Loss:     1.303637 Ones: 0.42 Accuracy: 0.51 Tokens per Sec:    13056, Lr: 0.000300
2019-12-08 17:55:43,550 Epoch   4 Step:      120 Batch Loss:     1.430135 Ones: 0.25 Accuracy: 0.42 Tokens per Sec:    26425, Lr: 0.000300
2019-12-08 17:55:44,246 Epoch   4 Step:      130 Batch Loss:     1.460399 Ones: 0.34 Accuracy: 0.45 Tokens per Sec:    36475, Lr: 0.000300
2019-12-08 17:55:44,420 Epoch   4: total training loss 45.34
2019-12-08 17:55:44,420 EPOCH 5
2019-12-08 17:55:44,944 Epoch   5 Step:      140 Batch Loss:     1.343282 Ones: 0.37 Accuracy: 0.53 Tokens per Sec:    11519, Lr: 0.000300
2019-12-08 17:55:45,625 Epoch   5 Step:      150 Batch Loss:     1.423782 Ones: 0.48 Accuracy: 0.60 Tokens per Sec:    20818, Lr: 0.000300
2019-12-08 17:55:46,321 Epoch   5 Step:      160 Batch Loss:     1.388378 Ones: 0.32 Accuracy: 0.46 Tokens per Sec:    32832, Lr: 0.000300
2019-12-08 17:55:46,692 Epoch   5: total training loss 44.55
2019-12-08 17:55:46,692 EPOCH 6
2019-12-08 17:55:47,016 Epoch   6 Step:      170 Batch Loss:     1.287259 Ones: 0.42 Accuracy: 0.56 Tokens per Sec:    11437, Lr: 0.000300
2019-12-08 17:55:47,658 Epoch   6 Step:      180 Batch Loss:     1.378858 Ones: 0.27 Accuracy: 0.42 Tokens per Sec:    17909, Lr: 0.000300
2019-12-08 17:55:48,266 Epoch   6 Step:      190 Batch Loss:     1.342870 Ones: 0.46 Accuracy: 0.54 Tokens per Sec:    30783, Lr: 0.000300
2019-12-08 17:55:48,966 Epoch   6: total training loss 44.04
2019-12-08 17:55:48,967 EPOCH 7
2019-12-08 17:55:49,088 Epoch   7 Step:      200 Batch Loss:     1.328706 Ones: 0.30 Accuracy: 0.42 Tokens per Sec:    11949, Lr: 0.000300
2019-12-08 17:55:51,880 Validation result at epoch   7, step      200: f1_prod:   0.11, loss:   0.9776, ones:   0.3395, f1_0:   0.2198, f1_1:   0.4889,f1_prd:   0.1075, duration: 2.7914s
2019-12-08 17:55:52,546 Epoch   7 Step:      210 Batch Loss:     1.305676 Ones: 0.52 Accuracy: 0.60 Tokens per Sec:    14321, Lr: 0.000300
2019-12-08 17:55:53,298 Epoch   7 Step:      220 Batch Loss:     1.375757 Ones: 0.30 Accuracy: 0.44 Tokens per Sec:    24708, Lr: 0.000300
2019-12-08 17:55:54,017 Epoch   7 Step:      230 Batch Loss:     1.275579 Ones: 0.57 Accuracy: 0.64 Tokens per Sec:    37663, Lr: 0.000300
2019-12-08 17:55:54,064 Epoch   7: total training loss 43.89
2019-12-08 17:55:54,065 EPOCH 8
2019-12-08 17:55:54,674 Epoch   8 Step:      240 Batch Loss:     1.161843 Ones: 0.36 Accuracy: 0.52 Tokens per Sec:    12000, Lr: 0.000300
2019-12-08 17:55:55,397 Epoch   8 Step:      250 Batch Loss:     1.066063 Ones: 0.53 Accuracy: 0.57 Tokens per Sec:    21972, Lr: 0.000300
2019-12-08 17:55:56,042 Epoch   8 Step:      260 Batch Loss:     1.261580 Ones: 0.44 Accuracy: 0.53 Tokens per Sec:    36579, Lr: 0.000300
2019-12-08 17:55:56,344 Epoch   8: total training loss 42.87
2019-12-08 17:55:56,345 EPOCH 9
2019-12-08 17:55:56,756 Epoch   9 Step:      270 Batch Loss:     1.244914 Ones: 0.48 Accuracy: 0.57 Tokens per Sec:    11817, Lr: 0.000300
2019-12-08 17:55:57,414 Epoch   9 Step:      280 Batch Loss:     1.221730 Ones: 0.42 Accuracy: 0.51 Tokens per Sec:    19932, Lr: 0.000300
2019-12-08 17:55:58,025 Epoch   9 Step:      290 Batch Loss:     1.391873 Ones: 0.35 Accuracy: 0.52 Tokens per Sec:    32856, Lr: 0.000300
2019-12-08 17:55:58,618 Epoch   9: total training loss 42.18
2019-12-08 17:55:58,618 EPOCH 10
2019-12-08 17:55:58,813 Epoch  10 Step:      300 Batch Loss:     1.114169 Ones: 0.55 Accuracy: 0.64 Tokens per Sec:    11089, Lr: 0.000300
2019-12-08 17:56:01,602 Hooray! New best validation result [eval_metric]!
2019-12-08 17:56:01,602 Saving new checkpoint.
2019-12-08 17:56:08,731 Validation result at epoch  10, step      300: f1_prod:   0.14, loss:   0.7411, ones:   0.5944, f1_0:   0.2004, f1_1:   0.7085,f1_prd:   0.1420, duration: 9.9174s
2019-12-08 17:56:09,550 Epoch  10 Step:      310 Batch Loss:     1.419011 Ones: 0.29 Accuracy: 0.44 Tokens per Sec:    14011, Lr: 0.000300
2019-12-08 17:56:10,275 Epoch  10 Step:      320 Batch Loss:     1.226983 Ones: 0.55 Accuracy: 0.63 Tokens per Sec:    28458, Lr: 0.000300
2019-12-08 17:56:10,853 Epoch  10 Step:      330 Batch Loss:     1.281582 Ones: 0.39 Accuracy: 0.46 Tokens per Sec:    47345, Lr: 0.000300
2019-12-08 17:56:10,854 Epoch  10: total training loss 41.63
2019-12-08 17:56:10,854 EPOCH 11
2019-12-08 17:56:11,580 Epoch  11 Step:      340 Batch Loss:     1.384765 Ones: 0.32 Accuracy: 0.46 Tokens per Sec:    12088, Lr: 0.000300
2019-12-08 17:56:12,295 Epoch  11 Step:      350 Batch Loss:     1.622136 Ones: 0.28 Accuracy: 0.50 Tokens per Sec:    24329, Lr: 0.000300
2019-12-08 17:56:12,862 Epoch  11 Step:      360 Batch Loss:     1.253338 Ones: 0.52 Accuracy: 0.62 Tokens per Sec:    42715, Lr: 0.000300
2019-12-08 17:56:13,109 Epoch  11: total training loss 40.75
2019-12-08 17:56:13,109 EPOCH 12
2019-12-08 17:56:13,540 Epoch  12 Step:      370 Batch Loss:     1.099836 Ones: 0.47 Accuracy: 0.64 Tokens per Sec:    12406, Lr: 0.000300
2019-12-08 17:56:14,134 Epoch  12 Step:      380 Batch Loss:     1.213713 Ones: 0.48 Accuracy: 0.58 Tokens per Sec:    20908, Lr: 0.000300
2019-12-08 17:56:15,033 Epoch  12 Step:      390 Batch Loss:     1.181534 Ones: 0.41 Accuracy: 0.55 Tokens per Sec:    25780, Lr: 0.000300
2019-12-08 17:56:15,380 Epoch  12: total training loss 40.26
2019-12-08 17:56:15,381 EPOCH 13
2019-12-08 17:56:15,702 Epoch  13 Step:      400 Batch Loss:     1.364930 Ones: 0.53 Accuracy: 0.63 Tokens per Sec:    10882, Lr: 0.000300
2019-12-08 17:56:18,487 Hooray! New best validation result [eval_metric]!
2019-12-08 17:56:18,487 Saving new checkpoint.
2019-12-08 17:56:25,723 Validation result at epoch  13, step      400: f1_prod:   0.14, loss:   0.7108, ones:   0.6380, f1_0:   0.1954, f1_1:   0.7381,f1_prd:   0.1443, duration: 10.0198s
2019-12-08 17:56:26,533 Epoch  13 Step:      410 Batch Loss:     1.265091 Ones: 0.34 Accuracy: 0.48 Tokens per Sec:    16402, Lr: 0.000300
2019-12-08 17:56:27,094 Epoch  13 Step:      420 Batch Loss:     1.230396 Ones: 0.48 Accuracy: 0.58 Tokens per Sec:    35746, Lr: 0.000300
2019-12-08 17:56:27,710 Epoch  13: total training loss 38.94
2019-12-08 17:56:27,711 EPOCH 14
2019-12-08 17:56:27,850 Epoch  14 Step:      430 Batch Loss:     1.257897 Ones: 0.45 Accuracy: 0.59 Tokens per Sec:    12235, Lr: 0.000300
2019-12-08 17:56:28,436 Epoch  14 Step:      440 Batch Loss:     1.010336 Ones: 0.57 Accuracy: 0.69 Tokens per Sec:    14426, Lr: 0.000300
2019-12-08 17:56:29,178 Epoch  14 Step:      450 Batch Loss:     1.365141 Ones: 0.44 Accuracy: 0.57 Tokens per Sec:    23390, Lr: 0.000300
2019-12-08 17:56:29,798 Epoch  14 Step:      460 Batch Loss:     1.699570 Ones: 0.37 Accuracy: 0.58 Tokens per Sec:    40020, Lr: 0.000300
2019-12-08 17:56:29,993 Epoch  14: total training loss 38.80
2019-12-08 17:56:29,994 EPOCH 15
2019-12-08 17:56:30,642 Epoch  15 Step:      470 Batch Loss:     1.120836 Ones: 0.63 Accuracy: 0.73 Tokens per Sec:    12124, Lr: 0.000300
2019-12-08 17:56:31,214 Epoch  15 Step:      480 Batch Loss:     1.092453 Ones: 0.49 Accuracy: 0.63 Tokens per Sec:    25618, Lr: 0.000300
2019-12-08 17:56:31,989 Epoch  15 Step:      490 Batch Loss:     1.143953 Ones: 0.47 Accuracy: 0.58 Tokens per Sec:    31128, Lr: 0.000300
2019-12-08 17:56:32,280 Epoch  15: total training loss 37.85
2019-12-08 17:56:32,280 EPOCH 16
2019-12-08 17:56:32,572 Epoch  16 Step:      500 Batch Loss:     1.047951 Ones: 0.59 Accuracy: 0.71 Tokens per Sec:    11523, Lr: 0.000300
2019-12-08 17:56:35,357 Validation result at epoch  16, step      500: f1_prod:   0.14, loss:   0.7472, ones:   0.6314, f1_0:   0.1958, f1_1:   0.7330,f1_prd:   0.1435, duration: 2.7853s
2019-12-08 17:56:36,003 Epoch  16 Step:      510 Batch Loss:     1.154017 Ones: 0.57 Accuracy: 0.69 Tokens per Sec:    16793, Lr: 0.000300
2019-12-08 17:56:36,760 Epoch  16 Step:      520 Batch Loss:     1.236723 Ones: 0.39 Accuracy: 0.49 Tokens per Sec:    26866, Lr: 0.000300
2019-12-08 17:56:37,346 Epoch  16: total training loss 36.99
2019-12-08 17:56:37,347 EPOCH 17
2019-12-08 17:56:37,544 Epoch  17 Step:      530 Batch Loss:     1.346853 Ones: 0.40 Accuracy: 0.54 Tokens per Sec:    11433, Lr: 0.000300
2019-12-08 17:56:38,135 Epoch  17 Step:      540 Batch Loss:     1.063944 Ones: 0.53 Accuracy: 0.65 Tokens per Sec:    15319, Lr: 0.000300
2019-12-08 17:56:38,851 Epoch  17 Step:      550 Batch Loss:     0.663522 Ones: 0.58 Accuracy: 0.72 Tokens per Sec:    24828, Lr: 0.000300
2019-12-08 17:56:39,535 Epoch  17 Step:      560 Batch Loss:     0.725509 Ones: 0.68 Accuracy: 0.75 Tokens per Sec:    37936, Lr: 0.000300
2019-12-08 17:56:39,640 Epoch  17: total training loss 35.50
2019-12-08 17:56:39,641 EPOCH 18
2019-12-08 17:56:40,285 Epoch  18 Step:      570 Batch Loss:     1.069171 Ones: 0.51 Accuracy: 0.63 Tokens per Sec:    12530, Lr: 0.000300
2019-12-08 17:56:40,949 Epoch  18 Step:      580 Batch Loss:     0.814972 Ones: 0.59 Accuracy: 0.70 Tokens per Sec:    23983, Lr: 0.000300
2019-12-08 17:56:41,649 Epoch  18 Step:      590 Batch Loss:     0.642289 Ones: 0.71 Accuracy: 0.82 Tokens per Sec:    34374, Lr: 0.000300
2019-12-08 17:56:41,925 Epoch  18: total training loss 34.74
2019-12-08 17:56:41,926 EPOCH 19
2019-12-08 17:56:42,361 Epoch  19 Step:      600 Batch Loss:     0.948654 Ones: 0.54 Accuracy: 0.70 Tokens per Sec:    12162, Lr: 0.000300
2019-12-08 17:56:45,153 Hooray! New best validation result [eval_metric]!
2019-12-08 17:56:45,154 Saving new checkpoint.
2019-12-08 17:56:52,407 Validation result at epoch  19, step      600: f1_prod:   0.15, loss:   0.7788, ones:   0.6159, f1_0:   0.2030, f1_1:   0.7198,f1_prd:   0.1462, duration: 10.0451s
2019-12-08 17:56:53,164 Epoch  19 Step:      610 Batch Loss:     1.133270 Ones: 0.42 Accuracy: 0.57 Tokens per Sec:    18817, Lr: 0.000300
2019-12-08 17:56:53,753 Epoch  19 Step:      620 Batch Loss:     1.100844 Ones: 0.55 Accuracy: 0.69 Tokens per Sec:    36249, Lr: 0.000300
2019-12-08 17:56:54,273 Epoch  19: total training loss 33.65
2019-12-08 17:56:54,273 EPOCH 20
2019-12-08 17:56:54,426 Epoch  20 Step:      630 Batch Loss:     0.809811 Ones: 0.55 Accuracy: 0.73 Tokens per Sec:     9484, Lr: 0.000300
2019-12-08 17:56:55,249 Epoch  20 Step:      640 Batch Loss:     1.494352 Ones: 0.46 Accuracy: 0.66 Tokens per Sec:    13396, Lr: 0.000300
2019-12-08 17:56:55,903 Epoch  20 Step:      650 Batch Loss:     0.914598 Ones: 0.60 Accuracy: 0.70 Tokens per Sec:    29135, Lr: 0.000300
2019-12-08 17:56:56,576 Epoch  20 Step:      660 Batch Loss:     0.683214 Ones: 0.68 Accuracy: 0.82 Tokens per Sec:    40673, Lr: 0.000300
2019-12-08 17:56:56,577 Epoch  20: total training loss 33.04
2019-12-08 17:56:56,577 EPOCH 21
2019-12-08 17:56:57,310 Epoch  21 Step:      670 Batch Loss:     1.063262 Ones: 0.56 Accuracy: 0.66 Tokens per Sec:    11710, Lr: 0.000300
2019-12-08 17:56:58,008 Epoch  21 Step:      680 Batch Loss:     1.135504 Ones: 0.56 Accuracy: 0.65 Tokens per Sec:    24604, Lr: 0.000300
2019-12-08 17:56:58,728 Epoch  21 Step:      690 Batch Loss:     1.113014 Ones: 0.55 Accuracy: 0.62 Tokens per Sec:    35622, Lr: 0.000300
2019-12-08 17:56:58,886 Epoch  21: total training loss 32.73
2019-12-08 17:56:58,886 EPOCH 22
2019-12-08 17:56:59,425 Epoch  22 Step:      700 Batch Loss:     1.038467 Ones: 0.53 Accuracy: 0.69 Tokens per Sec:    12225, Lr: 0.000300
2019-12-08 17:57:02,217 Validation result at epoch  22, step      700: f1_prod:   0.15, loss:   0.7770, ones:   0.6352, f1_0:   0.1993, f1_1:   0.7327,f1_prd:   0.1460, duration: 2.7916s
2019-12-08 17:57:02,985 Epoch  22 Step:      710 Batch Loss:     1.138683 Ones: 0.47 Accuracy: 0.60 Tokens per Sec:    20921, Lr: 0.000300
2019-12-08 17:57:03,582 Epoch  22 Step:      720 Batch Loss:     0.761619 Ones: 0.68 Accuracy: 0.80 Tokens per Sec:    38014, Lr: 0.000300
2019-12-08 17:57:03,998 Epoch  22: total training loss 31.66
2019-12-08 17:57:03,998 EPOCH 23
2019-12-08 17:57:04,326 Epoch  23 Step:      730 Batch Loss:     1.447280 Ones: 0.35 Accuracy: 0.59 Tokens per Sec:    11386, Lr: 0.000300
2019-12-08 17:57:04,997 Epoch  23 Step:      740 Batch Loss:     1.044236 Ones: 0.48 Accuracy: 0.63 Tokens per Sec:    17184, Lr: 0.000300
2019-12-08 17:57:05,656 Epoch  23 Step:      750 Batch Loss:     1.161487 Ones: 0.51 Accuracy: 0.66 Tokens per Sec:    29564, Lr: 0.000300
2019-12-08 17:57:06,287 Epoch  23: total training loss 30.17
2019-12-08 17:57:06,287 EPOCH 24
2019-12-08 17:57:06,396 Epoch  24 Step:      760 Batch Loss:     1.143222 Ones: 0.58 Accuracy: 0.68 Tokens per Sec:    12086, Lr: 0.000300
2019-12-08 17:57:07,154 Epoch  24 Step:      770 Batch Loss:     0.875382 Ones: 0.73 Accuracy: 0.80 Tokens per Sec:    13063, Lr: 0.000300
2019-12-08 17:57:07,783 Epoch  24 Step:      780 Batch Loss:     1.248851 Ones: 0.43 Accuracy: 0.58 Tokens per Sec:    27752, Lr: 0.000300
2019-12-08 17:57:08,413 Epoch  24 Step:      790 Batch Loss:     0.777685 Ones: 0.62 Accuracy: 0.75 Tokens per Sec:    40201, Lr: 0.000300
2019-12-08 17:57:08,573 Epoch  24: total training loss 30.45
2019-12-08 17:57:08,573 EPOCH 25
2019-12-08 17:57:09,254 Epoch  25 Step:      800 Batch Loss:     0.752716 Ones: 0.67 Accuracy: 0.77 Tokens per Sec:    11955, Lr: 0.000300
2019-12-08 17:57:12,043 Hooray! New best validation result [eval_metric]!
2019-12-08 17:57:12,044 Saving new checkpoint.
2019-12-08 17:57:19,292 Validation result at epoch  25, step      800: f1_prod:   0.15, loss:   0.8293, ones:   0.6112, f1_0:   0.2100, f1_1:   0.7252,f1_prd:   0.1523, duration: 10.0378s
2019-12-08 17:57:19,995 Epoch  25 Step:      810 Batch Loss:     0.781200 Ones: 0.65 Accuracy: 0.80 Tokens per Sec:    23157, Lr: 0.000300
2019-12-08 17:57:20,647 Epoch  25 Step:      820 Batch Loss:     0.982604 Ones: 0.73 Accuracy: 0.82 Tokens per Sec:    37640, Lr: 0.000300
2019-12-08 17:57:20,917 Epoch  25: total training loss 30.04
2019-12-08 17:57:20,917 Training ended after  25 epochs.
2019-12-08 17:57:20,917 Best validation result at step      800:   0.15 eval_metric.
