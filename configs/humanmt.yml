bert:
  path: 'bert-base-multilingual-cased'

data:
  source: '.en'
  target: '.hyp'
  marking: '.ann'
  raw: 'data/markings'
  train: 'data/markings.tok'
  dev: 'data/head.markings.tok'

model:
  hidden_dimension: 50
  activation: "tanh"
  freeze_bert: False
  head_bias: False

train:
  bert_lr: 0.000003
  lr: 0.0003
  optimizer: adam
  batch_size: 32
  epochs: 10
  seed: 42
  model_dir: humanmt
  shuffle: True
  cuda: False
  early_stopping_metric: 'eval_metric'
  overwrite: True
  normalization: tokens
  bad_weight: 5.0
  validation_freq: 100
  logging_freq: 10
  weighting: 'wrong'
  eval_batch_size: 128
  eval_metric: 'f1_prod' # f1_1, f1_0, acc, f1_prod

generate:
  batch_size: 2
