2019-12-08 12:18:06,801 Hello! This is AutoMark. For all your Automatic Marking needs.
2019-12-08 12:18:06,808 Total params: 177891990
2019-12-08 12:18:06,809 Trainable parameters: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'marking_head.fc1.bias', 'marking_head.fc1.weight', 'marking_head.prediction.weight']
2019-12-08 12:18:06,812 cfg.bert.path                      : bert-base-multilingual-cased
2019-12-08 12:18:06,812 cfg.data.source                    : .en
2019-12-08 12:18:06,812 cfg.data.target                    : .hyp
2019-12-08 12:18:06,812 cfg.data.marking                   : .ann
2019-12-08 12:18:06,813 cfg.data.raw_train                 : data/markings
2019-12-08 12:18:06,813 cfg.data.train                     : data/markings.tok
2019-12-08 12:18:06,813 cfg.data.raw_dev                   : data/user_mark
2019-12-08 12:18:06,813 cfg.data.dev                       : data/user_mark.tok
2019-12-08 12:18:06,813 cfg.model.hidden_dimension         : 50
2019-12-08 12:18:06,813 cfg.model.activation               : tanh
2019-12-08 12:18:06,813 cfg.model.freeze_bert              : False
2019-12-08 12:18:06,813 cfg.model.head_bias                : False
2019-12-08 12:18:06,813 cfg.train.bert_lr                  : 3e-06
2019-12-08 12:18:06,813 cfg.train.lr                       : 0.0003
2019-12-08 12:18:06,813 cfg.train.optimizer                : adam
2019-12-08 12:18:06,813 cfg.train.batch_size               : 32
2019-12-08 12:18:06,813 cfg.train.epochs                   : 10
2019-12-08 12:18:06,814 cfg.train.seed                     : 42
2019-12-08 12:18:06,814 cfg.train.model_dir                : humanmt
2019-12-08 12:18:06,814 cfg.train.shuffle                  : True
2019-12-08 12:18:06,814 cfg.train.cuda                     : False
2019-12-08 12:18:06,814 cfg.train.early_stopping_metric    : eval_metric
2019-12-08 12:18:06,814 cfg.train.overwrite                : True
2019-12-08 12:18:06,814 cfg.train.normalization            : tokens
2019-12-08 12:18:06,814 cfg.train.bad_weight               : 5.0
2019-12-08 12:18:06,814 cfg.train.validation_freq          : 100
2019-12-08 12:18:06,814 cfg.train.logging_freq             : 10
2019-12-08 12:18:06,814 cfg.train.weighting                : wrong
2019-12-08 12:18:06,814 cfg.train.eval_batch_size          : 128
2019-12-08 12:18:06,814 cfg.train.eval_metric              : f1_prod
2019-12-08 12:18:06,814 cfg.generate.batch_size            : 2
2019-12-08 12:18:06,816 AutoMark(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (marking_head): MarkingHead(
    (fc1): Linear(in_features=768, out_features=50, bias=True)
    (prediction): Linear(in_features=50, out_features=2, bias=False)
  )
)
2019-12-08 12:18:06,816 EPOCH 1
2019-12-08 12:18:48,508 Epoch   1 Step:       10 Batch Loss:     1.261032 Ones: 0.73 Accuracy: 0.68 Tokens per Sec:      217, Lr: 0.000300
2019-12-08 12:19:24,786 Epoch   1 Step:       20 Batch Loss:     0.991021 Ones: 0.83 Accuracy: 0.75 Tokens per Sec:      473, Lr: 0.000300
2019-12-08 12:20:01,669 Epoch   1 Step:       30 Batch Loss:     0.888961 Ones: 0.88 Accuracy: 0.78 Tokens per Sec:      678, Lr: 0.000300
2019-12-08 12:20:11,414 Epoch   1: total training loss 32.41
2019-12-08 12:20:11,416 EPOCH 2
2019-12-08 12:20:36,469 Epoch   2 Step:       40 Batch Loss:     0.792901 Ones: 0.93 Accuracy: 0.80 Tokens per Sec:      234, Lr: 0.000300
2019-12-08 12:21:10,719 Epoch   2 Step:       50 Batch Loss:     0.820005 Ones: 0.89 Accuracy: 0.79 Tokens per Sec:      402, Lr: 0.000300
2019-12-08 12:21:47,095 Epoch   2 Step:       60 Batch Loss:     0.699986 Ones: 0.89 Accuracy: 0.82 Tokens per Sec:      611, Lr: 0.000300
2019-12-08 12:22:11,259 Epoch   2: total training loss 28.19
2019-12-08 12:22:11,261 EPOCH 3
2019-12-08 12:22:25,816 Epoch   3 Step:       70 Batch Loss:     0.718736 Ones: 0.90 Accuracy: 0.82 Tokens per Sec:      206, Lr: 0.000300
2019-12-08 12:23:08,765 Epoch   3 Step:       80 Batch Loss:     0.675902 Ones: 0.93 Accuracy: 0.83 Tokens per Sec:      296, Lr: 0.000300
2019-12-08 12:23:40,478 Epoch   3 Step:       90 Batch Loss:     0.645368 Ones: 0.92 Accuracy: 0.84 Tokens per Sec:      620, Lr: 0.000300
2019-12-08 12:24:13,710 Epoch   3: total training loss 27.18
2019-12-08 12:24:13,711 EPOCH 4
2019-12-08 12:24:19,671 Epoch   4 Step:      100 Batch Loss:     0.920650 Ones: 0.78 Accuracy: 0.75 Tokens per Sec:      235, Lr: 0.000300
2019-12-08 12:25:05,567 Hooray! New best validation result [eval_metric]!
2019-12-08 12:25:05,568 Saving new checkpoint.
2019-12-08 12:25:25,757 Validation result at epoch   4, step      100: f1_prod:   0.13, loss:   0.6735, ones:   0.0078, f1_0:   0.1569, f1_1:   0.8577,f1_prd:   0.1345, duration: 66.0853s
2019-12-08 12:26:02,797 Epoch   4 Step:      110 Batch Loss:     0.893566 Ones: 0.80 Accuracy: 0.76 Tokens per Sec:      248, Lr: 0.000300
2019-12-08 12:26:41,858 Epoch   4 Step:      120 Batch Loss:     0.719881 Ones: 0.87 Accuracy: 0.81 Tokens per Sec:      461, Lr: 0.000300
2019-12-08 12:27:17,417 Epoch   4 Step:      130 Batch Loss:     0.941912 Ones: 0.82 Accuracy: 0.75 Tokens per Sec:      743, Lr: 0.000300
2019-12-08 12:27:22,048 Epoch   4: total training loss 26.61
2019-12-08 12:27:22,048 EPOCH 5
2019-12-08 12:27:55,346 Epoch   5 Step:      140 Batch Loss:     0.811982 Ones: 0.80 Accuracy: 0.78 Tokens per Sec:      234, Lr: 0.000300
2019-12-08 12:28:30,798 Epoch   5 Step:      150 Batch Loss:     0.642386 Ones: 0.90 Accuracy: 0.83 Tokens per Sec:      448, Lr: 0.000300
2019-12-08 12:29:04,555 Epoch   5 Step:      160 Batch Loss:     0.911166 Ones: 0.77 Accuracy: 0.76 Tokens per Sec:      686, Lr: 0.000300
2019-12-08 12:29:22,381 Epoch   5: total training loss 25.71
2019-12-08 12:29:22,383 EPOCH 6
2019-12-08 12:29:37,064 Epoch   6 Step:      170 Batch Loss:     0.551321 Ones: 0.94 Accuracy: 0.86 Tokens per Sec:      210, Lr: 0.000300
2019-12-08 12:30:18,906 Epoch   6 Step:      180 Batch Loss:     0.678979 Ones: 0.89 Accuracy: 0.82 Tokens per Sec:      315, Lr: 0.000300
2019-12-08 12:30:57,055 Epoch   6 Step:      190 Batch Loss:     0.593786 Ones: 0.99 Accuracy: 0.85 Tokens per Sec:      582, Lr: 0.000300
2019-12-08 12:31:19,477 Epoch   6: total training loss 26.57
2019-12-08 12:31:19,478 EPOCH 7
2019-12-08 12:31:28,028 Epoch   7 Step:      200 Batch Loss:     0.625813 Ones: 0.93 Accuracy: 0.84 Tokens per Sec:      262, Lr: 0.000300
2019-12-08 12:31:59,402 Validation result at epoch   7, step      200: f1_prod:   0.05, loss:   0.6695, ones:   0.0091, f1_0:   0.0511, f1_1:   0.9263,f1_prd:   0.0473, duration: 31.3724s
2019-12-08 12:32:32,569 Epoch   7 Step:      210 Batch Loss:     0.712355 Ones: 0.96 Accuracy: 0.82 Tokens per Sec:      304, Lr: 0.000300
2019-12-08 12:33:11,398 Epoch   7 Step:      220 Batch Loss:     1.043225 Ones: 0.67 Accuracy: 0.72 Tokens per Sec:      505, Lr: 0.000300
2019-12-08 12:33:36,802 Epoch   7 Step:      230 Batch Loss:     0.554367 Ones: 0.98 Accuracy: 0.86 Tokens per Sec:     1010, Lr: 0.000300
2019-12-08 12:33:42,976 Epoch   7: total training loss 26.54
2019-12-08 12:33:42,977 EPOCH 8
2019-12-08 12:34:10,818 Epoch   8 Step:      240 Batch Loss:     0.613912 Ones: 0.99 Accuracy: 0.85 Tokens per Sec:      255, Lr: 0.000300
2019-12-08 12:34:42,054 Epoch   8 Step:      250 Batch Loss:     0.614259 Ones: 1.00 Accuracy: 0.85 Tokens per Sec:      454, Lr: 0.000300
2019-12-08 12:35:22,415 Epoch   8 Step:      260 Batch Loss:     0.816992 Ones: 0.89 Accuracy: 0.78 Tokens per Sec:      606, Lr: 0.000300
2019-12-08 12:35:33,718 Epoch   8: total training loss 25.63
2019-12-08 12:35:33,719 EPOCH 9
2019-12-08 12:35:50,832 Epoch   9 Step:      270 Batch Loss:     0.406177 Ones: 0.93 Accuracy: 0.90 Tokens per Sec:      247, Lr: 0.000300
2019-12-08 12:36:32,996 Epoch   9 Step:      280 Batch Loss:     0.879980 Ones: 0.76 Accuracy: 0.76 Tokens per Sec:      346, Lr: 0.000300
2019-12-08 12:37:02,312 Epoch   9 Step:      290 Batch Loss:     0.569178 Ones: 0.99 Accuracy: 0.86 Tokens per Sec:      742, Lr: 0.000300
2019-12-08 12:37:24,996 Epoch   9: total training loss 24.85
2019-12-08 12:37:24,997 EPOCH 10
2019-12-08 12:37:35,938 Epoch  10 Step:      300 Batch Loss:     1.214625 Ones: 0.56 Accuracy: 0.67 Tokens per Sec:      273, Lr: 0.000300
2019-12-08 12:38:05,361 Validation result at epoch  10, step      300: f1_prod:   0.13, loss:   0.6857, ones:   0.0077, f1_0:   0.1495, f1_1:   0.8429,f1_prd:   0.1260, duration: 29.4215s
2019-12-08 12:38:45,213 Epoch  10 Step:      310 Batch Loss:     0.596788 Ones: 0.95 Accuracy: 0.85 Tokens per Sec:      322, Lr: 0.000300
2019-12-08 12:39:11,441 Epoch  10 Step:      320 Batch Loss:     0.796572 Ones: 0.74 Accuracy: 0.79 Tokens per Sec:      731, Lr: 0.000300
2019-12-08 12:39:42,578 Epoch  10 Step:      330 Batch Loss:     0.667362 Ones: 1.00 Accuracy: 0.83 Tokens per Sec:      878, Lr: 0.000300
2019-12-08 12:39:42,580 Epoch  10: total training loss 25.27
2019-12-08 12:39:42,580 Training ended after  10 epochs.
2019-12-08 12:39:42,580 Best validation result at step      100:   0.13 eval_metric.
